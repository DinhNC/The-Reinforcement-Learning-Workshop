{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the Reinforcement Learning Loop with Gym\n",
    "In this exercise we will implement a basic RL loop with episodes and timesteps using the Cart-Pole environment. You can change environment and use other environments as well, nothing changes as the main goal of gym is to unify the interfaces of all possible environments to build agents that are environment-agnostic as much as possible. This is very peculiar thing of RL: the algorithms are not usually suited to the task but are task-agnostic, so that they can be applied successfully to a variety of environments and still solve them. \n",
    "We need to create the environment as before. After that, we can loop for a defined number of episodes, for each episode we loop for a defined number of steps or until the episode is terminated (by checking the done value). For each timestep we have to call the env.step() function passing an action (we pass a random action for now), we collect the desired information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Number: 0, Timesteps: 34\n",
      "Episode Number: 1, Timesteps: 10\n",
      "Episode Number: 2, Timesteps: 12\n",
      "Episode Number: 3, Timesteps: 21\n",
      "Episode Number: 4, Timesteps: 16\n",
      "Episode Number: 5, Timesteps: 17\n",
      "Episode Number: 6, Timesteps: 12\n",
      "Episode Number: 7, Timesteps: 15\n",
      "Episode Number: 8, Timesteps: 16\n",
      "Episode Number: 9, Timesteps: 16\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# each episode is composed by 100 timesteps\n",
    "# define 1000 episodes\n",
    "n_episodes = 10\n",
    "n_timesteps = 100\n",
    "\n",
    "# loop for the episodes\n",
    "for episode_number in range(n_episodes):\n",
    "    # here we are inside an episode\n",
    "\n",
    "    # the reset function resets the environment and returns\n",
    "    # the first environment observation\n",
    "    observation = env.reset()\n",
    "\n",
    "    # loop for the given number of timesteps or\n",
    "    # until the episode is terminated\n",
    "    for timestep_number in range(n_timesteps):\n",
    "        \n",
    "        # render the environment\n",
    "        env.render(mode=\"rgb-array\")\n",
    "\n",
    "        # select the action\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        # apply the selected action by calling env.step\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        # if done the episode is terminated, we have to reset\n",
    "        # the environment\n",
    "        if done:\n",
    "            print(f\"Episode Number: {episode_number}, Timesteps: {timestep_number}\")\n",
    "            # break from the timestep loop\n",
    "            break\n",
    "\n",
    "# close the environment\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
