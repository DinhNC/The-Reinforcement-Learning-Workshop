{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 01.02\n",
    "\n",
    "Write the presented parameterizations in the case of a state composed by n components. In the first case the features can be represented by the identity function, in the second case the features are represented by a polynomial function of order 2. Use Numpy to implement all the requested policies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear\n",
    "\n",
    "$$\n",
    "\\pi(s) = \\theta^T \\phi(s)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearPolicy:\n",
    "    def __init__(self, parameters: np.ndarray, features: Callable[[np.ndarray], np.ndarray]):\n",
    "        \"\"\"\n",
    "        Linear Policy Constructor.\n",
    "        \n",
    "        Args:\n",
    "            parameters (np.ndarray): policy parameters as np.ndarray.\n",
    "            features (np.ndarray): features to be extracted from the state representation.\n",
    "        \"\"\"\n",
    "        self._parameters = parameters\n",
    "        self._features = features\n",
    "    \n",
    "    def __call__(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Call method of the Policy.\n",
    "        \n",
    "        Args:\n",
    "            state (np.ndarray): environment state.\n",
    "        \n",
    "        Returns:\n",
    "            The resulting action.\n",
    "        \"\"\"\n",
    "        \n",
    "        # calculate state features\n",
    "        state_features = self._features(state)\n",
    "        \n",
    "        # the parameters shape [0] should be the same as the state features\n",
    "        # as they must be multiplied\n",
    "        assert state_features.shape[0] == self._parameters.shape[0]\n",
    "        \n",
    "        # dot product between parameters and state features\n",
    "        return np.dot(self._parameters.T, state_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the defined policy in the case of state composed by a 5 dimensional array.\n",
    "Sample a random set of parameters and a random state vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.48515891]]\n"
     ]
    }
   ],
   "source": [
    "# sample a random set of parameters\n",
    "parameters = np.random.rand(5, 1)\n",
    "\n",
    "# define the state features as identity function\n",
    "features = lambda x: x\n",
    "\n",
    "# define the policy\n",
    "pi: LinearPolicy = LinearPolicy(parameters, features)\n",
    "    \n",
    "# sample a state\n",
    "state = np.random.rand(5, 1)\n",
    "\n",
    "# Call the policy obtaining the action\n",
    "action = pi(state)\n",
    "\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Policy\n",
    "\n",
    "$$\n",
    "\\pi(a | s, \\theta) = \\frac{1}{ \\sqrt{2 \\pi \\sigma_\\theta(\\phi(s))^2}} e^{-\\frac{(x-\\mu_{\\theta}(\\phi(s)))^2}{2  \\sigma_\\theta(\\phi(s))^2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianPolicy:\n",
    "    def __init__(self, mu_parameters: np.ndarray, sigma_parameters: np.ndarray, features: Callable[[np.ndarray], np.ndarray]):\n",
    "        \"\"\"\n",
    "        Linear Policy Constructor.\n",
    "        \n",
    "        Args:\n",
    "            mu_parameters (np.ndarray): policy parameters of the mean (\\mu) as np.ndarray.\n",
    "            sigma_parameters (np.ndarray): policy parameters of the standard deviation as np.ndarray.\n",
    "            features (np.ndarray): features to be extracted from the state representation.\n",
    "        \"\"\"\n",
    "        self._mu_parameters = mu_parameters\n",
    "        self._sigma_parameters = sigma_parameters\n",
    "        self._features = features\n",
    "    \n",
    "    def __call__(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Call method of the Policy.\n",
    "        \n",
    "        Args:\n",
    "            state (np.ndarray): environment state.\n",
    "        \n",
    "        Returns:\n",
    "            The action sampled from the policy distribution.\n",
    "        \"\"\"\n",
    "        \n",
    "        # calculate state features\n",
    "        state_features = self._features(state)\n",
    "        \n",
    "        # the parameters shape [0] should be the same as the state features\n",
    "        # as they must be multiplied\n",
    "        assert state_features.shape[0] == self._mu_parameters.shape[0]\n",
    "        assert state_features.shape[0] == self._sigma_parameters.shape[0]\n",
    "        \n",
    "        # dot product between parameters and state features\n",
    "        # \\mu is the mean of the gaussian\n",
    "        mu = np.dot(self._mu_parameters.T, state_features)\n",
    "        # the stddev (sigma) should be squared to avoid negative numbers\n",
    "        sigma = np.dot(self._sigma_parameters.T, state_features) ** 2\n",
    "        \n",
    "        # sample action from gaussian distribution\n",
    "        action = np.random.normal(mu, sigma)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def mu(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Mean of the distribution in the current state.\n",
    "        \n",
    "        Args:\n",
    "            state (np.ndarray): environment state.\n",
    "        \n",
    "        Returns:\n",
    "            mu: mean of the Gaussian distribution.\n",
    "        \"\"\"\n",
    "        # calculate state features\n",
    "        state_features = self._features(state)\n",
    "        \n",
    "        # the parameters shape [0] should be the same as the state features\n",
    "        # as they must be multiplied\n",
    "        assert state_features.shape[0] == self._mu_parameters.shape[0]\n",
    "        \n",
    "        return np.dot(self._mu_parameters.T, state_features)\n",
    "    \n",
    "    def sigma(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Stddev of the distribution in the current state.\n",
    "        \n",
    "        Args:\n",
    "            state (np.ndarray): environment state.\n",
    "            \n",
    "        Returns:\n",
    "            sigma: stddev of the Gaussian distribution.\n",
    "        \"\"\"\n",
    "        # calculate state features\n",
    "        state_features = self._features(state)\n",
    "        \n",
    "        # the parameters shape [0] should be the same as the state features\n",
    "        # as they must be multiplied\n",
    "        assert state_features.shape[0] == self._sigma_parameters.shape[0]\n",
    "        \n",
    "        return np.dot(self._sigma_parameters.T, state_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the defined policy in the case of state composed by a 5 dimensional array.\n",
    "Sample a random set of parameters and a random state vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.1072412]]\n"
     ]
    }
   ],
   "source": [
    "# sample a random set of parameters\n",
    "mu_parameters = np.random.rand(5, 1)\n",
    "sigma_parameters = np.random.rand(5, 1)\n",
    "\n",
    "# define the state features as identity function\n",
    "features = lambda x: x\n",
    "\n",
    "# define the policy\n",
    "pi: GaussianPolicy = GaussianPolicy(mu_parameters, sigma_parameters, features)\n",
    "    \n",
    "# sample a state\n",
    "state = np.random.rand(5, 1)\n",
    "\n",
    "# Call the policy obtaining the action\n",
    "action = pi(state)\n",
    "\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand better the current policy.\n",
    "Plot the resulting distribution defined by the current state and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1174e2550>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_samples = 100\n",
    "\n",
    "samples = []\n",
    "\n",
    "# sample action from the policy and append them to the samples array\n",
    "for _ in range(n_samples):\n",
    "    samples.append(pi(state).flatten()[0])\n",
    "\n",
    "# reduce the plot size\n",
    "plt.ylim(-1e-3, 0.5)\n",
    "# plot the samples on the x axis\n",
    "plt.scatter(samples, np.zeros_like(samples))    \n",
    "\n",
    "# plot the distribution\n",
    "mu = pi.mu(state).flatten()[0]\n",
    "sigma = pi.sigma(state).flatten()[0]\n",
    "x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n",
    "plt.plot(x, scipy.stats.norm.pdf(x, mu, sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the samples follow the probability density function of the gaussian distributions.\n",
    "Samples are concentrated in the mean of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boltzmann Policy\n",
    "\n",
    "$$\n",
    "\\pi(a_i | s) = \\frac{e^{\\theta_i^T \\phi(s)}}{\\sum_j e^{\\theta_j^T \\phi(s)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoltzmannPolicy:\n",
    "    def __init__(self, parameters: List[np.ndarray], features: Callable[[np.ndarray], np.ndarray]):\n",
    "        \"\"\"\n",
    "        Linear Policy Constructor.\n",
    "        \n",
    "        Args:\n",
    "            parameters (List[np.ndarray]): policy parameters for each action as np.ndarray.\n",
    "            features (np.ndarray): features to be extracted from the state representation.\n",
    "        \"\"\"\n",
    "        self._parameters = parameters\n",
    "        self._features = features\n",
    "        self._n_actions = len(self._parameters)\n",
    "    \n",
    "    def __call__(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Call method of the Policy.\n",
    "        \n",
    "        Args:\n",
    "            state (np.ndarray): environment state.\n",
    "            \n",
    "        Returns:\n",
    "            action: Action sampled from the action probabilities.\n",
    "        \"\"\"\n",
    "        \n",
    "        # calculate state features\n",
    "        state_features = self._features(state)\n",
    "        \n",
    "        # the parameters shape [0] should be the same as the state features\n",
    "        # as they must be multiplied\n",
    "        for action_params in self._parameters:\n",
    "            assert state_features.shape[0] == action_params.shape[0]\n",
    "        \n",
    "        # calculate scores for each action\n",
    "        scores = []\n",
    "        for action_params in self._parameters:\n",
    "            score = np.dot(action_params.T, state_features)[0, 0]\n",
    "            scores.append(score)\n",
    "        \n",
    "        # use scipy softmax function\n",
    "        action_probs = scipy.special.softmax(scores)\n",
    "        \n",
    "        # sample the action according to the probabilities\n",
    "        action = np.random.choice(self._n_actions, p=action_probs)\n",
    "        \n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the Boltzmann policy in the case of state composed by a 5 dimensional array.\n",
    "Sample a random set of parameters and a random state vector.\n",
    "Define number of action = 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Action is 1\n",
      "Selected action is 2\n",
      "Selected action is 2\n",
      "Selected action is 1\n",
      "Selected action is 0\n",
      "Selected action is 0\n",
      "Selected action is 2\n",
      "Selected action is 0\n",
      "Selected action is 2\n",
      "Selected action is 0\n",
      "Selected action is 2\n"
     ]
    }
   ],
   "source": [
    "n_actions = 3\n",
    "# sample a random set of parameters for each action\n",
    "parameters = [np.random.rand(5, 1) for _ in range(n_actions)]\n",
    "\n",
    "# define the state features as identity function\n",
    "features = lambda x: x\n",
    "\n",
    "# define the policy\n",
    "pi: BoltzmannPolicy = BoltzmannPolicy(parameters, features)\n",
    "    \n",
    "# sample a state\n",
    "state = np.random.rand(5, 1)\n",
    "\n",
    "# Call the policy obtaining the action\n",
    "action = pi(state)\n",
    "print(\"Selected Action is\", action)\n",
    "\n",
    "# sample some actions\n",
    "for _ in range(10):\n",
    "    print(\"Selected action is\", pi(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus\n",
    "\n",
    "Implement a quadratic state features and re-run the notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
