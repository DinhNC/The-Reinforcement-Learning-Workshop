{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Performance of a random agent\n",
    "This activity will guide you through an important phase of every RL experiment: the measurement of performances and the design of agents. You have to design a random agent using a Python class to modularize and keep the agent independent from the main loop. After that, you have to measure the mean and the variance of the Discounted Return using a batch of 100 episodes. You can use every environment you want, taking into account that the agentâ€™s action should be compatible with the environment. You can design two different types of agent for discrete action spaces and for continuous action spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Abstract class representing the agent\n",
    "Init with the action space and the function pi returning the action\n",
    "\"\"\"\n",
    "class Agent:\n",
    "    def __init__(self, action_space: gym.spaces.Space):\n",
    "        \"\"\"\n",
    "        Constructor of the agent class.\n",
    "        \n",
    "        Args:\n",
    "            action_space (gym.spaces.Space): environment action space\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"This class cannot be instantiated.\")\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def pi(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Agent's policy.\n",
    "        \n",
    "        Args:\n",
    "            state (np.ndarray): environment state\n",
    "        \n",
    "        Returns:\n",
    "            The selected action\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousAgent(Agent):\n",
    "    def __init__(self, action_space: gym.spaces.Space):\n",
    "\n",
    "        # check the action space type\n",
    "        if not isinstance(action_space, gym.spaces.Box):\n",
    "            raise ValueError(\"This is a Continuous Agent pass as input a Box Space.\")\n",
    "\n",
    "        # initialize the distribution according to the action space type\n",
    "        if (action_space.low == -np.inf) and (action_space.high == np.inf):\n",
    "            # the distribution is a normal distribution\n",
    "            self._pi = lambda: np.random.normal(loc=0, scale=1, size=action_space.shape)\n",
    "            return\n",
    "        if (action_space.low != -np.inf) and (action_space.high != np.inf):\n",
    "            # the distribution is a uniform distribution\n",
    "            self._pi = lambda: np.random.uniform(\n",
    "                low=action_space.low, high=action_space.high, size=action_space.shape\n",
    "            )\n",
    "            return\n",
    "        if action_space.low == -np.inf:\n",
    "            # negative exponential distribution\n",
    "            self._pi = (\n",
    "                lambda: -np.random.exponential(size=action_space.shape)\n",
    "                + action_space.high\n",
    "            )\n",
    "            return\n",
    "        if action_space.high == np.inf:\n",
    "            # exponential distribution\n",
    "            self._pi = (\n",
    "                lambda: np.random.exponential(size=action_space.shape)\n",
    "                + action_space.low\n",
    "            )\n",
    "            return\n",
    "\n",
    "    def pi(self, observation: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Policy: simply call the internal _pi().\n",
    "        \n",
    "        This is a random agent so the action is independent from the observation.\n",
    "        For real agents the action depends on the observation.\n",
    "        \"\"\"\n",
    "        return self._pi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteAgent(Agent):\n",
    "    def __init__(self, action_space: gym.spaces.Space):\n",
    "\n",
    "        # check the action space type\n",
    "        if not isinstance(action_space, gym.spaces.Discrete):\n",
    "            raise ValueError(\"This is a Discrete Agent pass as input a Discrete Space.\")\n",
    "\n",
    "        # initialize the distribution according to the action space n attribute\n",
    "        # the distribution is a uniform distribution\n",
    "        self._pi = lambda: np.random.randint(low=0, high=action_space.n)\n",
    "\n",
    "    def pi(self, observation: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Policy: simply call the internal _pi().\n",
    "        \n",
    "        This is a random agent so the action is independent from the observation.\n",
    "        For real agents the action depends on the observation.\n",
    "        \"\"\"\n",
    "        return self._pi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility function\n",
    "Utility function to initialize the correct agent based on the action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_agent(action_space: gym.spaces.Space):\n",
    "    \"\"\"\n",
    "    Returns the correct agent based on the action space type\n",
    "    \"\"\"\n",
    "    if isinstance(action_space, gym.spaces.Discrete):\n",
    "        return DiscreteAgent(action_space)\n",
    "    if isinstance(action_space, gym.spaces.Box):\n",
    "        return ContinuousAgent(action_space)\n",
    "    raise ValueError(\n",
    "        \"Only Box spaces or Discrete Spaces are allowed, check the action space of the environment\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Name\n",
    "env_name = \"CartPole-v0\"\n",
    "# Number of episodes\n",
    "episodes = 10\n",
    "# Number of Timesteps of each episodes\n",
    "timesteps = 100\n",
    "# Discount factor\n",
    "gamma = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed to show the environment in a notebook\n",
    "from gym import wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name)\n",
    "# the last argument is needed to record all episodes\n",
    "# otherwise gym would record only some of them\n",
    "# The monitor saves the episodes inside the folder ./gym-results\n",
    "env = wrappers.Monitor(env, \"./gym-results\", force=True, video_callable=lambda episode_id: True)\n",
    "\n",
    "agent = make_agent(env.action_space)\n",
    "\n",
    "# list of returns\n",
    "episode_returns = []\n",
    "\n",
    "# loop for the episodes\n",
    "for episode_number in range(episodes):\n",
    "    # here we are inside an episode\n",
    "\n",
    "    # reset cumulated gamma\n",
    "    gamma_cum = 1\n",
    "\n",
    "    # return of the current episode\n",
    "    episode_return = 0\n",
    "\n",
    "    # the reset function resets the environment and returns\n",
    "    # the first environment observation\n",
    "    observation = env.reset()\n",
    "\n",
    "    # loop for the given number of timesteps or\n",
    "    # until the episode is terminated\n",
    "    for timestep_number in range(timesteps):\n",
    "\n",
    "        # render the environment\n",
    "        env.render()\n",
    "\n",
    "        # select the action\n",
    "        action = agent.pi(observation)\n",
    "\n",
    "        # apply the selected action by calling env.step\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        # increment the return\n",
    "        episode_return += reward * gamma_cum\n",
    "\n",
    "        # update the value of cumulated discount factor\n",
    "        gamma_cum = gamma_cum * gamma\n",
    "\n",
    "        # if done the episode is terminated, we have to reset\n",
    "        # the environment\n",
    "        if done:\n",
    "            print(\n",
    "                f\"Episode Number: {episode_number}, Timesteps: {timestep_number}, Return: {episode_return}\"\n",
    "            )\n",
    "            # break from the timestep loop\n",
    "            break\n",
    "\n",
    "    episode_returns.append(episode_return)\n",
    "\n",
    "# close the environment\n",
    "env.close()\n",
    "\n",
    "# Calculate return statistics\n",
    "avg_return = np.mean(episode_returns)\n",
    "std_return = np.std(episode_returns)\n",
    "var_return = std_return ** 2  # variance is std^2\n",
    "\n",
    "print(f\"Statistics on Return: Average: {avg_return}, Variance: {var_return}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rendering\n",
    "\n",
    "Let's render the episodes inside a notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render the episodes\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "episodes_to_watch = 5\n",
    "for episode in range(episodes_to_watch):\n",
    "    video = io.open(f'./gym-results/openaigym.video.{env.file_infix}.video{episode:06d}.mp4', 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    display(HTML(data='''\n",
    "        <video width=\"360\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''\n",
    "    .format(encoded.decode('ascii'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the episode duration is not too long, this is because the actions are taken at random, thus the pole falls after some timesteps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
