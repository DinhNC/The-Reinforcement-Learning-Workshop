{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prerequisite:** OpenAI Baselines has to be installed for this Exercise to work. See section 4.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import all required modules from OpenAI baselines and Tensorflow to use PPO algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baselines.ppo2.ppo2 import learn\n",
    "from baselines.ppo2 import defaults\n",
    "from baselines.common.vec_env import VecEnv, VecFrameStack\n",
    "from baselines.common.cmd_util import make_vec_env, make_env\n",
    "from baselines.common.models import register\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define and register a custom Multi Layer Perceptron for the policy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register(\"custom_mlp\")\n",
    "def custom_mlp(num_layers=2, num_hidden=64, activation=tf.tanh):\n",
    "    \"\"\"\n",
    "    Stack of fully-connected layers to be used in a policy / q-function approximator\n",
    "    Parameters:\n",
    "    ----------\n",
    "    num_layers: int                 number of fully-connected layers (default: 2)\n",
    "    num_hidden: int                 size of fully-connected layers (default: 64)\n",
    "    activation:                     activation function (default: tf.tanh)\n",
    "    Returns:\n",
    "    -------\n",
    "    function that builds fully connected network with a given input tensor / placeholder\n",
    "    \"\"\"\n",
    "    def network_fn(input_shape):\n",
    "        print('input shape is {}'.format(input_shape))\n",
    "        x_input = tf.keras.Input(shape=input_shape)\n",
    "        h = x_input\n",
    "        for i in range(num_layers):\n",
    "            h = tf.keras.layers.Dense(units=num_hidden, name='custom_mlp_fc{}'.format(i),\n",
    "                                      activation=activation)(h)\n",
    "\n",
    "        network = tf.keras.Model(inputs=[x_input], outputs=[h])\n",
    "        network.summary()\n",
    "        return network\n",
    "\n",
    "    return network_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create a function to build the environment in the format required by OpenAI baselines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_env(env_id, env_type):\n",
    "\n",
    "    if env_type in {'atari', 'retro'}:\n",
    "        env = make_vec_env(env_id, env_type, 1, None, gamestate=None, reward_scale=1.0)\n",
    "        env = VecFrameStack(env, 4)\n",
    "\n",
    "    else:\n",
    "        env = make_vec_env(env_id, env_type, 1, None, reward_scale=1.0, flatten_dict_observations=True)\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Build the environment, choose policy network parameters and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = 'CartPole-v0'\n",
    "env_type = 'classic_control'\n",
    "print(\"Env type = \", env_type)\n",
    "\n",
    "env = build_env(env_id, env_type)\n",
    "\n",
    "hidden_nodes = 64\n",
    "hidden_layers = 2\n",
    "\n",
    "model = learn(network=\"custom_mlp\", env=env, total_timesteps=1e6, num_hidden=hidden_nodes, num_layers=hidden_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Run the trained agent in the environment and print the cumulative reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "if not isinstance(env, VecEnv):\n",
    "    obs = np.expand_dims(np.array(obs), axis=0)\n",
    "\n",
    "episode_rew = 0\n",
    "    \n",
    "while True:\n",
    "    actions, _, state, _ = model.step(obs)\n",
    "    obs, reward, done, info = env.step(actions.numpy())\n",
    "    if not isinstance(env, VecEnv):\n",
    "        obs = np.expand_dims(np.array(obs), axis=0)\n",
    "    env.render()\n",
    "    print(\"Reward = \", reward)\n",
    "    episode_rew += reward\n",
    "    \n",
    "    if done:\n",
    "        print('Episode Reward = {}'.format(episode_rew))\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Use the built-in OpenAI baseline `run` script to train PPO on `CartPole-v0` environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m baselines.run --alg=ppo2 --env=CartPole-v0 --num_timesteps=1e6 --save_path=./models/CartPole_2M_ppo2 --log_path=./logs/CartPole/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Use the built-in OpenAI Baseline `run` script to run the trained model on `CartPole-v0` environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m baselines.run --alg=ppo2 --env=CartPole-v0 --num_timesteps=0 --load_path=./models/CartPole_2M_ppo2 --play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Use the pretrained weights provided to see the trianed agent in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O cartpole_1M_ppo2.tar.gz https://github.com/PacktWorkshops/The-Reinforcement-Learning-Workshop/blob/master/Chapter04/cartpole_1M_ppo2.tar.gz?raw=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar xvzf cartpole_1M_ppo2.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m baselines.run --alg=ppo2 --env=CartPole-v0 --num_timesteps=0 --load_path=./cartpole_1M_ppo2 --play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
