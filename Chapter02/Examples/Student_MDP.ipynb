{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![student-mdp](../pictures/student-mdp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: calculate the state value function for an undecided student, i.e. a student with a random uniform policy for each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 6\n",
    "P_pi = np.zeros((n_states, n_states)) # transition matrix together with policy\n",
    "R = np.zeros_like(P_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State encoding:\n",
    "\n",
    "- 0: Class 1\n",
    "- 1: Class 2\n",
    "- 2: Class 3\n",
    "- 3: Social\n",
    "- 4: Pub\n",
    "- 5: Bed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the transition matrix considering a random policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_pi[0, 1] = 0.5\n",
    "P_pi[0, 3] = 0.5\n",
    "P_pi[1, 2] = 0.5\n",
    "P_pi[1, 5] = 0.5\n",
    "P_pi[2, 4] = 0.5\n",
    "P_pi[2, 5] = 0.5\n",
    "P_pi[4, 5] = 0.5\n",
    "P_pi[4, 0] = 0.5\n",
    "P_pi[3, 0] = 0.5\n",
    "P_pi[3, 3] = 0.5\n",
    "P_pi[5, 5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 0.5, 0. , 0.5, 0. , 0. ],\n",
       "       [0. , 0. , 0.5, 0. , 0. , 0.5],\n",
       "       [0. , 0. , 0. , 0. , 0.5, 0.5],\n",
       "       [0.5, 0. , 0. , 0.5, 0. , 0. ],\n",
       "       [0.5, 0. , 0. , 0. , 0. , 0.5],\n",
       "       [0. , 0. , 0. , 0. , 0. , 1. ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the reward matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "R[0, 1] = -2\n",
    "R[0, 3] = -1\n",
    "R[1, 2] = -2\n",
    "R[1, 5] = 0\n",
    "R[2, 4] = 15\n",
    "R[2, 5] = 10\n",
    "R[4, 5] = 10\n",
    "R[4, 0] = -10\n",
    "R[3, 3] = -1\n",
    "R[3, 0] = -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,  -2.,   0.,  -1.,   0.,   0.],\n",
       "       [  0.,   0.,  -2.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,  15.,  10.],\n",
       "       [ -3.,   0.,   0.,  -1.,   0.,   0.],\n",
       "       [-10.,   0.,   0.,   0.,   0.,  10.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the correctness of P\n",
    "assert((np.sum(P_pi, axis=1) == 1).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expected reward for each state\n",
    "R_expected = np.sum(P_pi * R, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.5],\n",
       "       [-1. ],\n",
       "       [12.5],\n",
       "       [-2. ],\n",
       "       [ 0. ],\n",
       "       [ 0. ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector R_expected contains the expected immediate reward for each state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to solve the Bellman Equation to find the value for each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now it is possible to solve the Bellman Equation\n",
    "gamma = 0.9\n",
    "A = np.eye(n_states, n_states) - gamma * P_pi\n",
    "B = R_expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve using scipy linalg\n",
    "V = linalg.solve(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.78587056],\n",
       "       [ 4.46226255],\n",
       "       [12.13836121],\n",
       "       [-5.09753046],\n",
       "       [-0.80364175],\n",
       "       [ 0.        ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the vector of state values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the image below for a graphical representation of the state values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![student-mdp](../pictures/student-mdp-solved-gamma-09.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the results change with $\\gamma = 0$, i.e., a myopic random student."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.5],\n",
       "       [-1. ],\n",
       "       [12.5],\n",
       "       [-2. ],\n",
       "       [ 0. ],\n",
       "       [ 0. ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = 0.\n",
    "A = np.eye(n_states, n_states) - gamma * P_pi\n",
    "B = R_expected\n",
    "# solve using scipy linalg\n",
    "V_gamma_zero = linalg.solve(A, B)\n",
    "V_gamma_zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![student-mdp](../pictures/student-mdp-solved-gamma-0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see using $\\gamma=0$ the value of each state it is exactly equal to the average reward according to the policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q function calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../pictures/reward_matrix.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_sa = np.zeros(((n_states-1)*2, 1))\n",
    "R_sa[0] = -2 # study in state 0\n",
    "R_sa[1] = -1 # social in state 0\n",
    "R_sa[2] = -2 # study in state 1\n",
    "R_sa[3] = 0 # sleep in state 1\n",
    "R_sa[4] = 10 # sleep in state 2\n",
    "R_sa[5] = 15 # beer in state 2\n",
    "R_sa[6] = -1 # social in state 3 (social)\n",
    "R_sa[7] = -3 # study in state 3 (social)\n",
    "R_sa[8] = 10 # sleep in state 4 (pub)\n",
    "R_sa[9] = -10 # study in state 4 (pub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_sa.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transition matrix contains the probability of landing in a given state starting from a state and an action. On the rows we have the source state and action, on the column the landing state.\n",
    "\n",
    "![transition_matrix](../pictures/transition_matrix.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.zeros(((n_states-1)*2, n_states)) # Transition Matrix (states x action, states)\n",
    "P[0, 1] = 1 # study in state 0 -> state 1\n",
    "P[1, 3] = 1 # social in state 0 -> state 3\n",
    "P[2, 2] = 1 # study in state 1 -> state 2\n",
    "P[3, 5] = 1 # sleep in state 1 -> state 5 (bed)\n",
    "P[4, 5] = 1 # sleep in state 2 -> state 5 (bed)\n",
    "P[5, 4] = 1 # beer in state 2 -> state 4 (pub)\n",
    "P[6, 3] = 1 # social in state 3 -> state 3 (social)\n",
    "P[7, 0] = 1 # study in state 3 -> state 0 (class1)\n",
    "P[8, 5] = 1 # sleep in state 4 -> state 5 (bed)\n",
    "P[9, 0] = 1 # study in state 4 -> state 0 (class 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the action value function using $\\gamma=0.9$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "Q_sa_pi = R_sa + gamma * P @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.01603629],\n",
       "       [ -5.58777741],\n",
       "       [  8.92452509],\n",
       "       [  0.        ],\n",
       "       [ 10.        ],\n",
       "       [ 14.27672242],\n",
       "       [ -5.58777741],\n",
       "       [ -4.60728351],\n",
       "       [ 10.        ],\n",
       "       [-11.60728351]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_sa_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q_sa_pi is the action value vector, for each couple state action we have the value of the action in that state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action value function is represented in the figure below. Action values are represented with $q_\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![student-mdp-q](../pictures/student-mdp-q.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.01603629,  -5.58777741],\n",
       "       [  8.92452509,   0.        ],\n",
       "       [ 10.        ,  14.27672242],\n",
       "       [ -5.58777741,  -4.60728351],\n",
       "       [ 10.        , -11.60728351]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape the column so that we obtain a vector with shape (n_states, n_actions)\n",
    "n_actions = 2\n",
    "Q_sa_pi2 = np.reshape(Q_sa_pi, (-1, n_actions))\n",
    "Q_sa_pi2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way, performing the argmax we obtain the index of the best action in each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_actions = np.reshape(np.argmax(Q_sa_pi2, -1), (-1, 1))\n",
    "best_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![student-mdp-best-actions](../pictures/student-mdp-q-best-actions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the image, the green arrows are the best actions in each state. We can easily find them by looking at the action maximizing the q function in each state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the action value calculation we can see that when $\\gamma=0$, the action value function is equal to the expected immediate reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -2.],\n",
       "       [ -1.],\n",
       "       [ -2.],\n",
       "       [  0.],\n",
       "       [ 10.],\n",
       "       [ 15.],\n",
       "       [ -1.],\n",
       "       [ -3.],\n",
       "       [ 10.],\n",
       "       [-10.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_sa_pi_gamma_zero = R_sa\n",
    "Q_sa_pi_gamma_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -2.,  -1.],\n",
       "       [ -2.,   0.],\n",
       "       [ 10.,  15.],\n",
       "       [ -1.,  -3.],\n",
       "       [ 10., -10.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_actions = 2\n",
    "Q_sa_pi_gamma_zero2 = np.reshape(Q_sa_pi_gamma_zero, (-1, n_actions))\n",
    "Q_sa_pi_gamma_zero2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_actions_gamma_zero = np.reshape(np.argmax(Q_sa_pi_gamma_zero2, -1), (-1, 1))\n",
    "best_actions_gamma_zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is visualized in the following figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![student-mdp-best-actions](../pictures/student-mdp-q-best-actions-gamma-zero.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to notice how the best actions are changed by modifying only the discount factor. Here, the best actions the agent can take, starting from Class 1 is \"Social\", as it provides a bigger immediate reward with respect to the action \"Study\". The action \"Social\" brings the agent in state \"Social\". Here, the best the agent can do is to repeat the action social, cumulating a negative rewards."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
