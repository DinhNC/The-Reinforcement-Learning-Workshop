{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import functools\n",
    "from rl.memory import Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQfDAgent:\n",
    "    def __init__(self, env, configuration,trajectories =None):\n",
    "        self.session = tf.InteractiveSession()\n",
    "        self.conf = configuration\n",
    "        \n",
    "        \n",
    "        self.replay = Memory(capacity=self.conf.replay_buffer_size, permanent_data=len(trajectories))\n",
    "        self.demos = Memory(capacity=self.conf.demo_buffer_size, permanent_data=self.conf.demo_buffer_size)\n",
    "        self.include_trajectories(trajectories= trajectories)  # add demo data to both demo_memory & replay_memory\n",
    "        self.step = 0\n",
    "        self.eps = self.conf.INITIAL_EPSILON\n",
    "        self.state_no = env.observation_space.shape[0]\n",
    "        self.action_no = env.action_space.n\n",
    "        \n",
    "        self.action_b \n",
    "        self.y_input\n",
    "        self.wweights \n",
    "        self.n_step_y_input \n",
    "        self.isdemo \n",
    "        self.eval_input\n",
    "        self.select_input\n",
    "\n",
    "        self.q_evaluation\n",
    "        self.q_choice\n",
    "\n",
    "        self.loss\n",
    "        self.optimizer\n",
    "        self.update\n",
    "        self.ae\n",
    "\n",
    "        self.save = tf.train.Saver()\n",
    "\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "      \n",
    "    #function to include transitions to demonstration memory\n",
    "    def include_tranjectories(self, trajectories):\n",
    "        for tra in trajectories :\n",
    "            self.demos.store(np.array(tra, dtype=object))\n",
    "            self.replay.store(np.array(tra, dtype=object))\n",
    "            assert len(tra) == 10\n",
    "\n",
    "#function for greedy policy\n",
    "    def greedy_act(self, current_state):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(0, self.action_no - 1)\n",
    "        return np.argmax(self.Q_select.eval([current_state])[0])\n",
    "    \n",
    "#function for training \n",
    "    def train_ahead(self):\n",
    "        for tr in range(self.conf.PRETRAIN_STEPS):\n",
    "            self.q_train(train=True)\n",
    "            if tr % 200 == 0 and tr > 0:\n",
    "                print('Training step with expert demonsrtations: {}'.format(tr))\n",
    "        self.step= 0\n",
    "        print('Finished demo training')\n",
    "\n",
    "#function for deep neural network layers\n",
    "    def neural_net_layers(self, current_state, cols, no_units1, no_units2, input_weight, input_bias, reg=None):\n",
    "        a_n = self.action_no\n",
    "        with tf.variable_scope('layer1'):\n",
    "            weight1 = tf.get_variable('weight1', [a_n, no_units1], initializer=input_weight, collections=cols, regularizer=reg)\n",
    "            bias1 = tf.get_variable('bias1', [1, no_units1], initializer=input_bias, collections=cols, regularizer=reg)\n",
    "            dense1 = tf.nn.relu(tf.matmul(current_state, weight1) + bias1)\n",
    "        with tf.variable_scope('layer2'):\n",
    "            weight2 = tf.get_variable('weight2', [no_units1, no_units2], initializer=input_weight, collections=cols, regularizer=reg)\n",
    "            bias2 = tf.get_variable('bias2', [1, no_units2], initializer=input_bias, collections=cols, regularizer=reg)\n",
    "            dense2 = tf.nn.relu(tf.matmul(dense1, weight2) + bias2)\n",
    "        with tf.variable_scope('layer3'):\n",
    "            weight3 = tf.get_variable('weight3', [no_units2, a_n], initializer=input_weight, collections=cols, regularizer=reg)\n",
    "            bias3 = tf.get_variable('bias3', [1, a_n], initializer=input_bias, collections=cols, regularizer=reg)\n",
    "            dense3 = tf.matmul(dense2, weight3) + bias3\n",
    "        return dense3\n",
    "\n",
    "    #function for selecting network\n",
    "    def select_q(self):\n",
    "        with tf.variable_scope('Network_selection') as scope:\n",
    "            cols = ['network_params_select', tf.GraphKeys.GLOBAL_VARIABLES]\n",
    "            input_weight = tf.random_uniform_initializer(-0.1, 0.1)\n",
    "            input_bias = tf.constant_initializer(0.1)\n",
    "            reg = tf.contrib.layers.l2_regularizer(scale=0.2)  \n",
    "            return self.build_layers(self.select_input, cols, 24, 24, input_weight, input_bias, reg)\n",
    "       \n",
    "    #function for evaluating the network \n",
    "    def eval_q(self):\n",
    "        with tf.variable_scope('Network_evaluation') as scope:\n",
    "            cols = ['network_params_evaluate', tf.GraphKeys.GLOBAL_VARIABLES]\n",
    "            input_weight = tf.random_uniform_initializer(-0.1, 0.1)\n",
    "            input_bias = tf.constant_initializer(0.1)\n",
    "            return self.build_layers(self.eval_input, cols, 24, 24, input_weight, input_bias)\n",
    "        \n",
    "\n",
    "    def loss_lin(self, ae, a):\n",
    "        return 0.0 if ae == a else 0.75\n",
    "\n",
    "    def loss_selec(self, select_q):\n",
    "        inp = 0.0\n",
    "        for i in range(self.conf.BATCH_SIZE):\n",
    "            ae = self.action_batch[i]\n",
    "            max_val = float(\"-inf\")\n",
    "            for act in range(self.action_n):\n",
    "                max_val = tf.max(select_q[i][act] + self.loss_lin(ae, act), max_val)\n",
    "            inp += self.isdemo[i] * (max_val - select_q[i][ae])\n",
    "        return inp\n",
    "\n",
    "   \n",
    "    def loss(self):\n",
    "        loss_dq = tf.reduce_mean(tf.squared_difference(self.select_q, self.y_input))\n",
    "        lloss_dq = tf.reduce_mean(tf.squared_difference(self.select_q, self.n_step_y_input))\n",
    "        loss_inp = self.loss_selec(self.select_q)\n",
    "        loss_l2 = tf.reduce_sum([tf.reduce_mean(reg_l) for reg_l in tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)])\n",
    "        return self.weights * tf.reduce_sum([l * lam for l, lam in zip([loss_dq, lloss_dq, loss_inp, loss_l2], self.conf.LAMBDA)])\n",
    "\n",
    "\n",
    "    def abs_er(self):\n",
    "        return tf.reduce_sum(tf.abs(self.y_input - self.select_q), axis=1)  \n",
    "\n",
    " \n",
    "    def optimization(self):\n",
    "        optimizer = tf.train.AdamOptimizer(self.conf.LEARNING_RATE)\n",
    "        return optimizer.minimize(self.loss)  \n",
    "    \n",
    "\n",
    "    def update_target(self):\n",
    "        select_p = tf.get_collection('network_params_select')\n",
    "        eval_p = tf.get_collection('network_params_evaluate')\n",
    "        return [tf.assign(e, s) for e, s in zip(eval_p, select_p)]\n",
    "\n",
    "#     def save_model(self):\n",
    "#         print(\"Model saved in : {}\".format(self.saver.save(self.sess, self.config.MODEL_PATH)))\n",
    "\n",
    "#     def restore_model(self):\n",
    "#         self.saver.restore(self.sess, self.config.MODEL_PATH)\n",
    "#         print(\"Model restored.\")\n",
    "\n",
    "    def perceive(self, trajectory):\n",
    "        self.replay.store(np.array(trajectory))\n",
    "        \n",
    "        if self.replay.full():\n",
    "            self.epsilon = max(self.conf.FINAL_EPSILON, self.epsilon * self.conf.EPSILIN_DECAY)\n",
    "\n",
    "    def train_nework(self, train =False, update=True):\n",
    "       \n",
    "        if self.replay.full() or train==True:\n",
    "            return \n",
    "        self.step= self.step+ 1\n",
    "\n",
    "        assert self.replay.full() or train\n",
    "\n",
    "        actual_mem = self.demos if train else self.replay\n",
    "        tree_idx, minibatch, wweights = actual_mem.sample(self.conf.BATCH_SIZE)\n",
    "\n",
    "        np.random.shuffle(minibatch)\n",
    "        current_state_batch = [data[0] for data in minibatch]\n",
    "        action_batch = [data[1] for data in minibatch]\n",
    "        reward_batch = [data[2] for data in minibatch]\n",
    "        new_state_batch = [data[3] for data in minibatch]\n",
    "        done_batch = [data[4] for data in minibatch]\n",
    "        demos_data = [data[5] for data in minibatch]\n",
    "        nth_step_reward_batch = [data[6] for data in minibatch]\n",
    "        nth_step_state_batch = [data[7] for data in minibatch]\n",
    "        nth_step_done_batch = [data[8] for data in minibatch]\n",
    "        actual_no = [data[9] for data in minibatch]\n",
    "\n",
    "        # provide for placeholderï¼Œcompute first\n",
    "        select_q = self.select_q.eval(new_state_batch)\n",
    "        eval_q = self.eval_q.eval(new_state_batch)\n",
    "        n_step_select_q = self.select_q.eval(nth_step_state_batch)\n",
    "        n_step_eval_q = self.eval_q.eval(nth_step_state_batch)\n",
    "\n",
    "        y_batch = np.zeros((self.conf.BATCH_SIZE, self.action_no))\n",
    "        n_step_y_batch = np.zeros((self.conf.BATCH_SIZE, self.action_no))\n",
    "        for i in range(self.config.BATCH_SIZE):\n",
    "\n",
    "            temp = self.select_q.eval( current_state_batch[i].reshape((-1, self.state_dim)))[0]\n",
    "            temp_0 = np.copy(temp)\n",
    "            # add 1-step reward\n",
    "            action = np.argmax(select_q[i])\n",
    "            temp[action_batch[i]] = reward_batch[i] + (1 - int(done_batch[i])) * self.conf.GAMMA * eval_q[i][action]\n",
    "            y_batch[i] = temp\n",
    "            # add n-step reward\n",
    "            action = np.argmax(n_step_select_q[i])\n",
    "            q_nth_step = (1 - int(nth_step_done_batch[i])) * self.config.GAMMA**actual_no[i] * n_step_eval_q[i][action]\n",
    "            temp_0[action_batch[i]] = nth_step_reward_batch[i] + q_nth_step\n",
    "            n_step_y_batch[i] = temp_0\n",
    "\n",
    "        _, abs_errors = self.session.run([self.optimize, self.abs_errors],\n",
    "                                      y_batch,\n",
    "                                      n_step_y_batch,\n",
    "                                      current_state_batch,\n",
    "                                      action_batch,\n",
    "                                      demos_data,\n",
    "                                      wweights)\n",
    "\n",
    "\n",
    "     \n",
    "        if update and self.step % self.conf.UPDATE_TARGET_NET == 0:\n",
    "            self.session.run(self.update_target_net)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}