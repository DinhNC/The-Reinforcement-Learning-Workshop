{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#import packages\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import random as r\n",
    "import math as m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import neural network charactristics from keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#initialise memory class\n",
    "class MemoryRL:\n",
    "    # initialisation function\n",
    "    def __init__(self, mem_max):\n",
    "        self.mem_max = mem_max\n",
    "        self.samps = []\n",
    "\n",
    "    # appending function\n",
    "    def append_sample(self, sample):\n",
    "        self.samps.append(sample)\n",
    "        if len(self.samps) > self.mem_max:\n",
    "            self.samps.pop(0)\n",
    "\n",
    "    # sample generation function\n",
    "    def generate(self, samp_nos):\n",
    "        if samp_nos > len(self.samps):\n",
    "            return r.sample(self.samps, len(self.samps))\n",
    "        else:\n",
    "            return r.sample(self.samps, samp_nos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class AgentRL:\n",
    "    # agent parameters initialisation\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.memory = MemoryRL(5000)\n",
    "        self.model = self.create_model()\n",
    "        self.target = self.create_model()\n",
    "\n",
    "        # initialise parameters\n",
    "        self.epsilon_max = 1\n",
    "        self.epsilon_min = 0.01\n",
    "        self.gamma_p = 0.99\n",
    "        self.batch_size = 50\n",
    "        self.lamda = 0.0001\n",
    "        self.tau = 0.099\n",
    "        \n",
    "    #define deep neural network model\n",
    "    def create_model(self):\n",
    "        agent = Sequential()\n",
    "        s_shape = self.env.observation_space.shape\n",
    "        agent.add(Dense(50, input_dim=s_shape[0], activation=\"relu\"))\n",
    "        agent.add(Dense(50, activation=\"relu\"))\n",
    "        agent.add(Dense(self.env.action_space.n))\n",
    "        agent.compile(loss=\"mean_squared_error\", optimizer=Adam(lr=0.02))\n",
    "        return agent\n",
    "    \n",
    "    #create function for action selection\n",
    "    def select_action(self, state, step):\n",
    "        epsilon = self.epsilon_min + (self.epsilon_max - self.epsilon_min)* m.exp(-self.lamda *step)\n",
    "        if np.random.random() < epsilon:\n",
    "            return r.randint(0, self.env.action_space.n - 1)\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state)[0])\n",
    "    \n",
    "    #create function for updating the agent information\n",
    "    def capture_memory(self, state, action, reward, new_state, done):\n",
    "        self.memory.append_sample((state, action, reward, new_state, done))\n",
    "        \n",
    "    #create function for q-updates\n",
    "    def replay(self):\n",
    "        batch_size = self.batch_size\n",
    "        samples = self.memory.generate(batch_size)\n",
    "        for sample in samples:\n",
    "            state, action, reward, new_state, done = sample\n",
    "            Q_val = self.target.predict(state)\n",
    "            next_Q = self.target.predict(new_state)\n",
    "            if done:\n",
    "                Q_val[0][action] = reward\n",
    "            else:\n",
    "                next_Q = np.amax(next_Q)\n",
    "                Q_val[0][action] = reward + next_Q * self.gamma_p\n",
    "            self.model.fit(state, Q_val, epochs=1, verbose=0)\n",
    "    \n",
    "    #create function for updating the weights of the network\n",
    "    def train(self):\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target.get_weights()\n",
    "        for i in range(len(target_weights)):\n",
    "            target_weights[i] = weights[i] * self.tau + target_weights[i] * (1 - self.tau)\n",
    "        self.target.set_weights(target_weights)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 of 10\n"
     ]
    }
   ],
   "source": [
    "#main function\n",
    "if __name__ == \"__main__\":\n",
    "    #generate the mountain car model\n",
    "    env = gym.make(\"MountainCar-v0\")\n",
    "    trials = 10\n",
    "    trial_len = 200\n",
    "    dqn_agent = AgentRL(env=env)\n",
    "    count_trials = 0\n",
    "    stored_reward = []\n",
    "    max_state = -100\n",
    "    stored_f_state = []\n",
    "    \n",
    "    #for loop for episodes initilise state and reward\n",
    "    for trial in range(trials):\n",
    "        if trials % 10 == 0:\n",
    "            print(\"Episode {} of {}\".format(trial+1, trials))\n",
    "        current_state = env.reset()\n",
    "        current_state= np.reshape(current_state,[1, 2])\n",
    "        total_reward = 0\n",
    "        step_count = 0\n",
    "        # for loop for steps, update action and pass unformation to env. \n",
    "        for step in range(trial_len):\n",
    "            env.render()\n",
    "            action = dqn_agent.select_action(current_state, step + 1)\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            #reset reward values as positive rewards for states closer to the optimum\n",
    "            if new_state[0] >= -0.5:\n",
    "                reward += 1\n",
    "            elif new_state[0] >= -0.1:\n",
    "                reward += 5\n",
    "            elif new_state[0] >= 0.1:\n",
    "                reward += 10\n",
    "            elif new_state[0] >= 0.25:\n",
    "                reward += 20\n",
    "            elif new_state[0] >= 0.5:\n",
    "                reward += 100\n",
    "\n",
    "            if new_state[0] > max_state:\n",
    "                max_state = new_state[0]\n",
    "            new_state = np.reshape(new_state,[1, 2])\n",
    "\n",
    "            #update memory with newly generated values, replay and train\n",
    "            dqn_agent.capture_memory(current_state, action, reward, new_state, done)\n",
    "            dqn_agent.replay()\n",
    "            dqn_agent.train()\n",
    "            \n",
    "            #update move to the new state unless the end of the episode\n",
    "            current_state = new_state\n",
    "            total_reward += reward\n",
    "            step_count += 1\n",
    "            if done:\n",
    "                break\n",
    "        #update total reward and final state printout steps and reward    \n",
    "        stored_reward.append(total_reward)\n",
    "        stored_f_state.append(max_state)\n",
    "        print(\"Steps {}, total reward {}\".format(step_count, total_reward))\n",
    "    #plot the final states acheived and rewards for the episodes\n",
    "    plt.plot(stored_f_state)\n",
    "    plt.show()\n",
    "    plt.close(\"all\")\n",
    "    plt.plot(stored_reward)\n",
    "    plt.show()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": [
     "#Mountain Car Activity by a.petre\n",
     "\n",
     "#import packages\n",
     "import gym\n",
     "import numpy as np\n",
     "import tensorflow as tf\n",
     "import matplotlib.pylab as plt\n",
     "import random as r\n",
     "import math as m\n",
     "\n",
     "from keras.models import Sequential\n",
     "from keras.layers import Dense, Activation, Flatten\n",
     "from keras.optimizers import Adam\n",
     "\n",
     "from rl.agents.dqn import DQNAgent\n",
     "from rl.policy import EpsGreedyQPolicy\n",
     "\n",
     "#initialise parameters\n",
     "epsilon_max = 1\n",
     "epsilon_min = 0.01\n",
     "lambda = 0.001\n",
     "gamma = 0.99\n",
     "batch_size = 50\n",
     "\n",
     "#use memory\n",
     "class Mem:\n",
     "\n",
     "\n",
     "\n",
     "\n",
     "\n",
     "\n"
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
